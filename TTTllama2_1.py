# -*- coding: utf-8 -*-
"""TrumpTuring5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QGh1BQ1MWlY6Vw1iabjXC9rrtp2XI0HD
"""

!pip install -q transformers datasets accelerate peft bitsandbytes sentencepiece huggingface_hub
!pip install -q pandas numpy tqdm

# Check GPU availability
import torch
print(f"CUDA available: {torch.cuda.is_available()}")
print(f"GPU device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}")

import os
import pandas as pd
import numpy as np
import torch
from datasets import Dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments,
    pipeline,
    logging
)
from peft import (
    prepare_model_for_kbit_training,
    LoraConfig,
    get_peft_model,
    PeftModel
)
from tqdm import tqdm

# Set random seed for reproducibility
torch.manual_seed(42)

from huggingface_hub import login

hf_token = input("Enter your Hugging Face token: ")
login(token=hf_token)

# Load the dataset
df = pd.read_csv("midterm_trump.csv")
print(f"Original dataset size: {len(df)}")

# Display the first few rows to understand the data structure
df.head()

# Filter out retweets based on the isRetweet column
# First check if 't' or 'true' values exist in the isRetweet column
print("Unique values in isRetweet column:", df['isRetweet'].unique())

# Remove retweets
df_clean = df[df['isRetweet'].str.lower() != 't']
df_clean = df_clean[df_clean['isRetweet'].str.lower() != 'true']
print(f"Dataset size after removing retweets: {len(df_clean)}")

# Check for missing values in the text column
print(f"Number of missing text values: {df_clean['text'].isna().sum()}")

# Remove rows with missing text
df_clean = df_clean.dropna(subset=['text'])
print(f"Final dataset size: {len(df_clean)}")

# View a sample of the cleaned texts
print("\nSample tweets:")
for i, text in enumerate(df_clean['text'].sample(5).tolist()):
    print(f"{i+1}. {text}")

df_clean = df_clean.sample(frac=0.3, random_state=42)
print(f"Using reduced dataset of {len(df_clean)} examples for faster training")

# Function to format tweets for training
def format_for_training(examples):
    texts = examples['text']
    # Format as instruction-tuning format for better control
    formatted_texts = [
        f"Generate a tweet in Donald Trump's style: {text}"
        for text in texts
    ]
    return formatted_texts

# Convert to Hugging Face dataset
texts = format_for_training({'text': df_clean['text'].tolist()})
dataset = Dataset.from_dict({'text': texts})

# Split dataset into training and validation
dataset = dataset.shuffle(seed=42)
split_dataset = dataset.train_test_split(test_size=0.1)

print(f"Training examples: {len(split_dataset['train'])}")
print(f"Validation examples: {len(split_dataset['test'])}")

# View a sample from the training set
print("\nSample formatted training example:")
print(split_dataset['train'][0]['text'])

# Load Llama 2 model
model_id = "meta-llama/Llama-2-7b-chat-hf"  # Replaced TinyLlama with Llama 2

# Configure BitsAndBytes for efficient training
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_id, token=hf_token)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

# Load model
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    quantization_config=bnb_config,
    device_map="auto",
    token=hf_token
)

# Prepare model for training
model = prepare_model_for_kbit_training(model)

# LoRA configuration
peft_config = LoraConfig(
    r=8,
    lora_alpha=16,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=[
        "q_proj",
        "k_proj",
        "v_proj",
        "o_proj",
    ]  # removed MLP modules
)

model = get_peft_model(model, peft_config)

# Tokenization function
def tokenize_function(examples):
    return tokenizer(
        examples["text"],
        padding="max_length",
        truncation=True,
        max_length=256,  # Increased from 128 to 256 for Llama 2
        return_tensors=None
    )

# Tokenize datasets
tokenized_train = split_dataset["train"].map(
    tokenize_function,
    batched=True,
    remove_columns=["text"]
)

tokenized_val = split_dataset["test"].map(
    tokenize_function,
    batched=True,
    remove_columns=["text"]
)

print(f"Sample tokenized training example: {tokenized_train[0]}")

# Set training arguments
training_args = TrainingArguments(
    output_dir="./results_trump_llama2",
    num_train_epochs=1,  # Increased epochs
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    gradient_accumulation_steps=2,  # Increased gradient accumulation
    eval_strategy="steps",
    eval_steps=10,  # Evaluating more frequently
    logging_steps=10,
    learning_rate=2e-4,  # Slightly lower learning rate for stability
    weight_decay=0.01,
    fp16=True,  # Use mixed precision
    save_steps=20,
    save_total_limit=2,
    load_best_model_at_end=True,
    report_to="none",
    push_to_hub=False,
    gradient_checkpointing=True,  # Gradient checkpointing to save memory
    optim="adamw_torch_fused"  # Using fused optimizer for better performance
)

# Import Trainer
from transformers import Trainer, DataCollatorForLanguageModeling

# Data collator
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False
)

# Create trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_val,
    data_collator=data_collator,
)

print(f"Model parameters being trained: {model.print_trainable_parameters()}")

trainer.train()

from google.colab import drive
drive.mount('/content/drive')

drive_dir = "/content/drive/MyDrive/trump_tweet_checkpoint"
!mkdir -p {drive_dir}

!cp -r {current_checkpoint}/* {drive_dir}/
print(f"Checkpoint saved to Google Drive at {drive_dir}")

# Save model and tokenizer
output_dir = "./trump_tweet_model_llama2"
model.save_pretrained(output_dir)
tokenizer.save_pretrained(output_dir)
print(f"Model saved to {output_dir}")

def generate_trump_tweet(topic, max_length=150):
    """
    Generate a tweet in Trump's style on a given topic using Llama 2

    Args:
        topic (str): The topic for the tweet
        max_length (int): Maximum length of the generated tweet

    Returns:
        str: Generated tweet
    """
    # Format the prompt for Llama 2
    prompt = f"Generate a tweet in Donald Trump's style about {topic}:"

    # Tokenize the prompt
    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")

    # Generate the tweet
    with torch.no_grad():
        outputs = model.generate(
            input_ids=inputs["input_ids"],
            attention_mask=inputs["attention_mask"],
            max_new_tokens=150,
            temperature=0.9,
            top_p=0.92,
            top_k=50,
            repetition_penalty=1.2,
            do_sample=True,
            num_return_sequences=1,
        )

    # Decode the generated text
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # Clean up the generated text to extract just the tweet
    tweet = generated_text.replace(prompt, "").strip()

    return tweet

# Test topics
test_topics = [
    "the economy",
    "fake news",
    "China trade",
    "immigration",
    "election"
]

# Generate tweets
print("Generated Trump-style tweets:\n")
for topic in test_topics:
    tweet = generate_trump_tweet(topic)
    print(f"Topic: {topic}")
    print(f"Tweet: {tweet}")
    print("-" * 50)

# Save the trained model and tokenizer
output_dir = "./trump_tweet_model_llama2"
model.save_pretrained(output_dir)
tokenizer.save_pretrained(output_dir)
print(f"Model saved to {output_dir}")

# Check what was saved
!ls -la {output_dir}

from google.colab import drive
drive.mount('/content/drive')

drive_dir = "/content/drive/MyDrive/trump_tweet_model"
!mkdir -p {drive_dir}

# Copy the model files to Google Drive
!cp -r {output_dir}/* {drive_dir}/
print(f"Model saved to Google Drive at {drive_dir}")

# Verify the files were copied
!ls -la {drive_dir}

# Create a zip file of the model
!zip -r trump_tweet_model.zip {output_dir}
print("Model zipped for download. Use the file browser to download it.")

# Simple function to generate tweets interactively
def interactive_tweet_generator():
    while True:
        topic = input("\nEnter a topic for a Trump-style tweet (or 'quit' to exit): ")
        if topic.lower() == 'quit':
            break

        tweet = generate_trump_tweet(topic)
        print(f"\nGenerated Trump-style tweet about '{topic}':")
        print("-" * 50)
        print(tweet)
        print("-" * 50)

# Run the interactive generator
interactive_tweet_generator()

## saved code to reload to new instance if needed, not ran in this Juptyer notebook

##

##

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel

from google.colab import drive
drive.mount('/content/drive')


base_model_id = "meta-llama/Llama-2-7b-chat-hf"
hf_token = input("Enter your Hugging Face token: ")


bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)


base_model = AutoModelForCausalLM.from_pretrained(
    base_model_id,
    quantization_config=bnb_config,
    device_map="auto",
    token=hf_token
)


adapter_path = "/content/drive/MyDrive/trump_tweet_model"

model = PeftModel.from_pretrained(base_model, adapter_path)

tokenizer = AutoTokenizer.from_pretrained(base_model_id, token=hf_token)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

print("Trump tweet generator model loaded successfully!")

def generate_trump_tweet(topic, max_length=150):
    """Generate a tweet in Trump's style on a given topic"""
    prompt = f"Generate a tweet in Donald Trump's style about {topic}:"
    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")

    with torch.no_grad():
        outputs = model.generate(
            input_ids=inputs["input_ids"],
            attention_mask=inputs["attention_mask"],
            max_new_tokens=150,
            temperature=0.9,
            top_p=0.92,
            top_k=50,
            repetition_penalty=1.2,
            do_sample=True,
            num_return_sequences=1,
        )

    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    tweet = generated_text.replace(prompt, "").strip()
    return tweet

test_topics = ["the economy", "fake news", "immigration"]
for topic in test_topics:
    tweet = generate_trump_tweet(topic)
    print(f"Topic: {topic}")
    print(f"Tweet: {tweet}")
    print("-" * 50)