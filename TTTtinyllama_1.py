# -*- coding: utf-8 -*-
"""TrumpTuring3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Hy2EcQOXqQtnERU6JTTYldnjIVFxARxC
"""

# Install required libraries
!pip install -q transformers datasets accelerate peft bitsandbytes sentencepiece huggingface_hub
!pip install -q pandas numpy tqdm

# Check GPU availability
import torch
print(f"CUDA available: {torch.cuda.is_available()}")
print(f"GPU device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}")

import os
import pandas as pd
import numpy as np
import torch
from datasets import Dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments,
    pipeline,
    logging
)
from peft import (
    prepare_model_for_kbit_training,
    LoraConfig,
    get_peft_model,
    PeftModel
)
from tqdm import tqdm

# Set random seed for reproducibility
torch.manual_seed(42)

from huggingface_hub import login

# Input Hugging Face token
hf_token = input("Enter your Hugging Face token: ")
login(token=hf_token)

# Load the dataset
df = pd.read_csv("midterm_trump.csv")
print(f"Original dataset size: {len(df)}")

# Display the first few rows
df.head()

# Filter out retweets based on the isRetweet column
# First check if 't' or 'true' values exist in the isRetweet column
print("Unique values in isRetweet column:", df['isRetweet'].unique())

# Remove retweets
df_clean = df[df['isRetweet'].str.lower() != 't']
df_clean = df_clean[df_clean['isRetweet'].str.lower() != 'true']
print(f"Dataset size after removing retweets: {len(df_clean)}")

# Check for missing values in the text column
print(f"Number of missing text values: {df_clean['text'].isna().sum()}")

# Remove rows with missing text
df_clean = df_clean.dropna(subset=['text'])
print(f"Final dataset size: {len(df_clean)}")

# View a sample of the cleaned texts to verify
print("\nSample tweets:")
for i, text in enumerate(df_clean['text'].sample(5).tolist()):
    print(f"{i+1}. {text}")

# Use 20% of the data for test run
df_clean = df_clean.sample(frac=0.2, random_state=42)
print(f"Using reduced dataset of {len(df_clean)} examples for faster training")

# Function to format tweets for training
def format_for_training(examples):
    texts = examples['text']
    # Format as instruction-tuning format for better control
    formatted_texts = [
        f"Generate a tweet in Donald Trump's style: {text}"
        for text in texts
    ]
    return formatted_texts

# Convert to Hugging Face dataset
texts = format_for_training({'text': df_clean['text'].tolist()})
dataset = Dataset.from_dict({'text': texts})

# Split dataset into training and validation
dataset = dataset.shuffle(seed=42)
split_dataset = dataset.train_test_split(test_size=0.1)

print(f"Training examples: {len(split_dataset['train'])}")
print(f"Validation examples: {len(split_dataset['test'])}")

# View a sample from the training set
print("\nSample formatted training example:")
print(split_dataset['train'][0]['text'])

# Loading open-access TinyLlama model instead of Llama 2
model_id = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"

# Configure BitsAndBytes for efficient training
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_id)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

# Load model
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    quantization_config=bnb_config,
    device_map="auto"
)

# Prepare model for training
model = prepare_model_for_kbit_training(model)

# LoRA configuration
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=[
        "q_proj",
        "k_proj",
        "v_proj",
        "o_proj"
    ]
)

# Create PEFT model
model = get_peft_model(model, peft_config)

# Tokenization function
def tokenize_function(examples):
    return tokenizer(
        examples["text"],
        padding="max_length",
        truncation=True,
        max_length=128,
        return_tensors=None
    )

# Tokenize datasets
tokenized_train = split_dataset["train"].map(
    tokenize_function,
    batched=True,
    remove_columns=["text"]
)

tokenized_val = split_dataset["test"].map(
    tokenize_function,
    batched=True,
    remove_columns=["text"]
)

print(f"Sample tokenized training example: {tokenized_train[0]}")

# Setting training arguments
training_args = TrainingArguments(
    output_dir="./results_trump_llama2",
    num_train_epochs=1,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    gradient_accumulation_steps=2,
    evaluation_strategy="steps",
    eval_steps=500,
    logging_steps=50,
    learning_rate=2e-4,
    weight_decay=0.01,
    fp16=True,
    save_steps=500,
    save_total_limit=1,
    load_best_model_at_end=True,
    report_to="none",
    push_to_hub=False
)

# Import Trainer
from transformers import Trainer, DataCollatorForLanguageModeling

# Data collator
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False
)

# Create trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_val,
    data_collator=data_collator,
)

print(f"Model parameters being trained: {model.print_trainable_parameters()}")

# Start training
trainer.train()

# Save model and tokenizer
output_dir = "./trump_tweet_model_tinyllama"
model.save_pretrained(output_dir)
tokenizer.save_pretrained(output_dir)
print(f"Model saved to {output_dir}")

def generate_trump_tweet(topic, max_length=100):
    """
    Generate a tweet in Trump's style on a given topic

    Args:
        topic (str): The topic for the tweet
        max_length (int): Maximum length of the generated tweet

    Returns:
        str: Generated tweet
    """
    # Format the prompt
    prompt = f"Generate a tweet in Donald Trump's style about {topic}:"

    # Tokenize the prompt
    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")

    # Generate the tweet
    with torch.no_grad():
        outputs = model.generate(
            input_ids=inputs["input_ids"],
            attention_mask=inputs["attention_mask"],
            max_length=max_length,
            temperature=0.9,
            top_p=0.92,
            top_k=50,
            repetition_penalty=1.2,
            do_sample=True,
            num_return_sequences=1,
        )

    # Decode the generated text
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # Clean up the generated text to extract just the tweet
    tweet = generated_text.replace(prompt, "").strip()

    return tweet

# Test topics
test_topics = [
    "the economy",
    "fake news",
    "China trade",
    "immigration",
    "election"
]

# Generate tweets
print("Generated Trump-style tweets:\n")
for topic in test_topics:
    tweet = generate_trump_tweet(topic)
    print(f"Topic: {topic}")
    print(f"Tweet: {tweet}")
    print("-" * 50)

# Simple function to generate tweets interactively
def interactive_tweet_generator():
    while True:
        topic = input("\nEnter a topic for a Trump-style tweet (or 'quit' to exit): ")
        if topic.lower() == 'quit':
            break

        tweet = generate_trump_tweet(topic)
        print(f"\nGenerated Trump-style tweet about '{topic}':")
        print("-" * 50)
        print(tweet)
        print("-" * 50)

# Run the interactive generator
interactive_tweet_generator()